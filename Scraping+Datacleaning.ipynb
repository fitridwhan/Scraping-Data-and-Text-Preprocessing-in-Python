{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2t2zi0s4BfAV5T8r7IaxY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fitridwhan/Scraping-Data-and-Text-Preprocessing-in-Python/blob/main/Scraping%2BDatacleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL GOOGLE PLAY SCRAPER"
      ],
      "metadata": {
        "id": "uF9J35EGc6YO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3qs8Dn-KEO-",
        "outputId": "85386041-5973-4e4f-9fb1-88851c2a3505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-play-scraper in /usr/local/lib/python3.7/dist-packages (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-play-scraper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import library python"
      ],
      "metadata": {
        "id": "Eb7srkBKdAtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import app\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ubCyEImSLc8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape All available reviews \n",
        "#(DONT RUN THIS CELL!!! RUN ONLY IF YOU WANT TO SCRAPE ALL AVAILABLE REVIEWS)\n",
        "#JANGAN DI RUN KODE INI, KECUALI ANDA INGIN SCRAPE SEMUA REVIEWS SEKALIGUS\n",
        "\n",
        "from google_play_scraper import Sort, reviews_all\n",
        "\n",
        "result = reviews_all(\n",
        "    'net.zenius.mobile',\n",
        "    sleep_milliseconds=0, # defaults to 0\n",
        "    lang='id', # defaults to 'en'\n",
        "    country='id', # defaults to 'us'\n",
        "    sort=Sort.NEWEST, # defaults to Sort.MOST_RELEVANT , you can use Sort.NEWEST to get newst reviews\n",
        ")\n"
      ],
      "metadata": {
        "id": "0AociVvYKktm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_baru = pd.DataFrame(np.array(result),columns=['score'])\n",
        "\n",
        "df_baru = df_baru.join(pd.DataFrame(df_baru.pop('score').tolist()))\n",
        "\n",
        "df_baru"
      ],
      "metadata": {
        "id": "BAXAo97MK0W9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_baru.index)"
      ],
      "metadata": {
        "id": "rZUVAx6iL65C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df[\"score\"].value_counts()"
      ],
      "metadata": {
        "id": "USSUw0RKMJr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df[\"content\"].value_counts()"
      ],
      "metadata": {
        "id": "XqmFjk3QMZRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KInIncrPSdqC",
        "outputId": "aa87bce4-dc95-426b-b093-967ca3aa84be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mentah = pd.read_excel('data2019dan2022.xlsx', header=0)\n",
        "df_mentah #kita panggil data"
      ],
      "metadata": {
        "id": "PVKRBejeSgqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "bNRYV6OOSsPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df[\"score\"].value_counts()"
      ],
      "metadata": {
        "id": "v7-7wCNbEx2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df.describe()"
      ],
      "metadata": {
        "id": "Xbu8RZU0S8Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#casefolding proses\n",
        "def clean_lower(lwr):\n",
        "    lwr = lwr.lower() # lowercase text\n",
        "    return lwr\n",
        "# Buat kolom tambahan untuk data description yang telah dicasefolding  \n",
        "my_df['lwr'] = my_df['content'].apply(clean_lower)\n",
        "casefolding=pd.DataFrame(my_df['lwr'])\n",
        "casefolding"
      ],
      "metadata": {
        "id": "vdEpKzEeTCT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "o6qM_YV8TGPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datacleaning 1\n",
        "#Remove Puncutuation atau delete tanda baca\n",
        "clean_spcl = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "clean_symbol = re.compile('[^0-9a-z]')\n",
        "def clean_punct(text):\n",
        "    text = clean_spcl.sub('', text)\n",
        "    text = clean_symbol.sub(' ', text)\n",
        "    return text\n",
        "# Buat kolom tambahan untuk data description yang telah diremovepunctuation   \n",
        "my_df['clean_punct'] = my_df['lwr'].apply(clean_punct)\n",
        "my_df['clean_punct']"
      ],
      "metadata": {
        "id": "W6y_cmjvTRWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "gQS_1JXdTUjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean_number\n",
        "clean_symbol = re.compile('[\"\\d+\", \"\"]')\n",
        "def clean_punct(text):\n",
        "    text = clean_spcl.sub('', text)\n",
        "    text = clean_symbol.sub(' ', text)\n",
        "    return text\n",
        "    \n",
        "# Buat kolom tambahan untuk data description yang telah diremovepunctuation   \n",
        "my_df['clean_number'] = my_df['clean_punct'].apply(clean_punct)\n",
        "my_df['clean_number']"
      ],
      "metadata": {
        "id": "bDRbtlubTmZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "5Nthl1JMTpvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df.drop_duplicates(subset =\"clean_number\", keep= 'first', inplace = True)"
      ],
      "metadata": {
        "id": "_xuYqBTHVK92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "wBEUG8x1VpS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "LV749-rCWhxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ('stemmer')"
      ],
      "metadata": {
        "id": "B6HviuZbYwB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "regexp = RegexpTokenizer('\\w+')\n",
        "\n",
        "my_df['text_token']= my_df['clean_number'].apply(regexp.tokenize)"
      ],
      "metadata": {
        "id": "kQJA6dFNZYDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "AVMy_mP5Xx-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('indonesian')\n",
        "print = stopwords.words('indonesian')"
      ],
      "metadata": {
        "id": "vD_8kY-hbYLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nltk_data(text):\n",
        "    t = [token for token in text if token.lower() not in stopwords.words(\"indonesian\")]\n",
        "    text = ' '.join(t)    \n",
        "    return text"
      ],
      "metadata": {
        "id": "zFk0a70ibgyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stopword removal\n",
        "my_df['Reviews_wo_Stop_Words'] = my_df['text_token'].apply(nltk_data)\n",
        "my_df['Reviews_wo_Stop_Words']"
      ],
      "metadata": {
        "id": "pn82X8XkblJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "uXBT1BS1b28v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords = factory.get_stop_words()"
      ],
      "metadata": {
        "id": "UbaCpkpIcH1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing after stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "regexp = RegexpTokenizer('\\w+')\n",
        "\n",
        "my_df['token_StopWord']= my_df['Reviews_wo_Stop_Words'].apply(regexp.tokenize)"
      ],
      "metadata": {
        "id": "nBRMAZ6YcMnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "RWXavrjocdrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df.to_csv(\"2019dan2022.csv\", index = False)"
      ],
      "metadata": {
        "id": "LNPmPvYhccUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##PorterStemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "##LancasterStemmer\n",
        "ls = nltk.LancasterStemmer()"
      ],
      "metadata": {
        "id": "8du-VlWScoT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import Sastrawi package\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()"
      ],
      "metadata": {
        "id": "sdTAfRT2cr_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "_TZN3VYHcxHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import Sastrawi package\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# stemmed\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in my_df['token_StopWord']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "            \n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)"
      ],
      "metadata": {
        "id": "wG3Bka9Xc2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "id": "-yPUE00odAQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "metadata": {
        "id": "C8gfHzeodCqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter\n",
        "import swifter"
      ],
      "metadata": {
        "id": "JbdleD7xdFm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "my_df['tokens_stemmed'] = my_df['token_StopWord'].swifter.apply(get_stemmed_term)\n",
        "my_df['tokens_stemmed']"
      ],
      "metadata": {
        "id": "wLWyek_udMpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "sTleuaUpdVHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "SRLx-ZUtebmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/u/0/uc?id=1Wm0ywH-nuBlQqYXWra883k0CsWauugp1&export=download'\n",
        "gdown.download(url, 'new_kamusalay.csv', quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "vKMHPy4weg6n",
        "outputId": "8fe5aba1-9137-4efd-b855-467dabdf4505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1Wm0ywH-nuBlQqYXWra883k0CsWauugp1&export=download\n",
            "To: /content/new_kamusalay.csv\n",
            "100%|██████████| 287k/287k [00:00<00:00, 38.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'new_kamusalay.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alay = pd.read_csv(\"/content/new_kamusalay.csv\")"
      ],
      "metadata": {
        "id": "Wvejiuw1ejvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = my_df['tokens_stemmed']"
      ],
      "metadata": {
        "id": "pe6QxXRYemd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(my_df['tokens_stemmed'])):\n",
        "  def stemmed_wrapper(term):\n",
        "    n = my_df['tokens_stemmed'][i].split(' ')\n",
        "    for a in range(len(alay['alay'])):\n",
        "      if alay['alay'][a] in n:\n",
        "        n[n.index(alay['alay'][a])] = alay['normal'][a]\n",
        "        my_df['tokens_stemmed'][i] = ' ' .join(n)\n",
        "        for i in range(len(my_df['tokens_stemmed'])):\n",
        "          my_df['tokens_stemmed'][i] = \" \".join(w for w in data['tokens_stemmed'][i].split() if not any(c.isdigit() for c in w))"
      ],
      "metadata": {
        "id": "QxY6srX-eooQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df"
      ],
      "metadata": {
        "id": "NYEDeiCxeyPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df.info('tokens_stemmed')"
      ],
      "metadata": {
        "id": "VXJej4cYew-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_df.isna().sum()"
      ],
      "metadata": {
        "id": "aevnWoQEnapN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = pd.read_excel('data proses.xlsx', header=0)\n",
        "df_clean #kita panggil data"
      ],
      "metadata": {
        "id": "Wi6CdHcttJ6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "akhir_df = df_clean[['Rating','Comment']]"
      ],
      "metadata": {
        "id": "WnGoPkYAtfJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "akhir_df[\"Rating\"].value_counts()"
      ],
      "metadata": {
        "id": "pCDM8IyJtZqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "akhir_df.isna().sum()"
      ],
      "metadata": {
        "id": "xECP7aNf0z2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}